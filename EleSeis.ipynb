{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TA2ydwxA2Sg2"
      },
      "outputs": [],
      "source": [
        "dropbox_url = \"https://www.dropbox.com/sh/p1swf94hs2pa47g/AAAXce5SfgWKizq7rpTM7wxna/dset_allspec_150?preview=dset_allspec_150_spectrograms.nc&subfolder_nav_tracking=1&st=wvoql63r&dl=1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "local_filename1 = '/content/old_data/spectzip.nc'"
      ],
      "metadata": {
        "id": "5X3r-kd92WGS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/content/old_data' , exist_ok = True)"
      ],
      "metadata": {
        "id": "zrdDGDSTCcXR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, local_filename):\n",
        "    try:\n",
        "        # Download the file from the URL and save it locally\n",
        "        urllib.request.urlretrieve(url, local_filename)\n",
        "        print(f\"Downloaded: {local_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download. Error: {e}\")\n",
        "\n",
        "# Download the file\n",
        "download_file(dropbox_url, local_filename1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BMBdt5e2a7V",
        "outputId": "ff03205a-ddec-4551-cb96-37e55a804e35"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: /content/old_data/spectzip.nc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/old_data/spectzip.nc -d /content/old_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkQHMdSXPPfG",
        "outputId": "92b1f648-e570-40ea-da12-0d89743994e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/old_data/spectzip.nc\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            " extracting: /content/old_data/seis_dict.pkl  \n",
            " extracting: /content/old_data/class_dict.pkl  \n",
            " extracting: /content/old_data/dset_allspec_150_info.txt  \n",
            " extracting: /content/old_data/dset_allspec_150_chunks.csv  \n",
            " extracting: /content/old_data/dset_allspec_150_chunks_clean.nc  "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete clean chunks file , when disk size reach maximum as we are only gonna work with spectrograms .\n"
      ],
      "metadata": {
        "id": "PiBaD5wUHtGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.remove('/content/old_data/spectzip.nc')"
      ],
      "metadata": {
        "id": "Bw3YEF5f2elt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netCDF4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPlcgu-F_awA",
        "outputId": "251eba5b-ea91-4f45-c1c8-a1d40526ba68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netCDF4\n",
            "  Downloading netCDF4-1.7.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from netCDF4) (2024.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from netCDF4) (1.25.2)\n",
            "Installing collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.4 netCDF4-1.7.1.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucaTn8kc5EBj",
        "outputId": "fae4fffb-a8e8-4197-966c-a372d24545f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy('/content/drive/MyDrive/8julyseis/spect.nc' , '/content/old_data/dset_allspec_150_spectrograms.nc')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rkUN9nF461EM",
        "outputId": "e6ed998d-0cff-4fdd-e55e-26dd4aada59b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/old_data/dset_allspec_150_spectrograms.nc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from netCDF4 import Dataset\n",
        "import numpy as np\n",
        "\n",
        "# Define the file path within Google Drive\n",
        "file_path = '/content/old_data/dset_allspec_150_spectrograms.nc'\n",
        "\n",
        "# Create a new NetCDF file\n",
        "ncfile = Dataset(file_path, 'r')\n",
        "print(ncfile)\n",
        "ncfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaVdi8m76Hk9",
        "outputId": "fa937412-769d-4cab-fe17-6b67454a5e04"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'netCDF4._netCDF4.Dataset'>\n",
            "root group (NETCDF4 data model, file format HDF5):\n",
            "    dimensions(sizes): side(128), traces(16831), comp(3)\n",
            "    variables(dimensions): float32 mel_spectr(traces, side, side, comp), int32 class(traces), float32 distance(traces), int64 seis(traces)\n",
            "    groups: class_dict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/old_data/dset_allspec_150_info.txt\" , 'r' ) as file :\n",
        "  file_content = file.read()\n",
        "  print(file_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpVu4bcb98JF",
        "outputId": "806dfd7a-9d40-4fc8-ad6e-8e46a02e6d5c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum distance from seismometer: 150.00 meters \n",
            "Maximum number of animals: 100 \n",
            "Consider only images with single species? True\n",
            "Behaviours considered: \n",
            "['Running', 'Walking', 'approach', 'charge', 'riding', 'running', 'saunter', 'waking', 'waling', 'walkig', 'walkiing', 'walkin', 'walking', 'walking.', 'walkng', 'wrestle']\n",
            "Species considered: \n",
            "['ELEPHANTIDAE', 'EQUIDAE', 'BOVIDAE', 'THRESKIORNITHIDAE', 'GIRAFFIDAE', 'SUIDAE', 'HOMINIDAE', 'CAMELIDAE', 'LEPORIDAE', 'NUMIDIDAE', 'FELIDAE', 'HYAENIDAE', 'MURIDAE', 'MALACONOTIDAE', 'MUSTELIDAE', 'ESTRILDIDAE', 'CANIDAE', 'HERPESTIDAE', 'HIPPOPOTAMIDAE', 'COLUMBIDAE', 'CERCOPITHECIDAE', 'STURNIDAE', 'ORYCTEROPODIDAE', 'CISTICOLIDAE']\n",
            "Class dictionary: \n",
            "{'HOMINIDAE': 1, 'ELEPHANTIDAE': 2, 'SUIDAE': 3, 'FELIDAE': 4, 'NUMIDIDAE': 5, 'BOVIDAE': 6, 'GIRAFFIDAE': 7, 'HYAENIDAE': 8, 'LEPORIDAE': 9, 'EQUIDAE': 10, 'MUSTELIDAE': 11, 'HIPPOPOTAMIDAE': 12, 'noise': 0}\n",
            "Class counts: \n",
            "1     71321\n",
            "6      5014\n",
            "0      2978\n",
            "5      1743\n",
            "9       548\n",
            "4       437\n",
            "2       212\n",
            "7       130\n",
            "8       110\n",
            "3        86\n",
            "10       10\n",
            "11        4\n",
            "Name: class, dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_obj(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "iJ6we0MWjgtm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "InputParams = {}\n",
        "InputParams[\"LONG_DIR\"] = \"/content/old_data\"\n",
        "InputParams[\"NEW_DIR\"] = \"/content/red_data\"\n",
        "InputParams[\"DSET_NAMES\"] = [\"'dset_allspec_150_spectrograms.nc'\"] #[\"dset_allspec_150_spectrograms.nc\"]\n",
        "InputParams[\"DISTANCE\"] = 50\n",
        "InputParams[\"Datafile\"] = 'dset_allspec_150_spectrograms.nc'\n",
        "InputParams[\"DATA_PATH\"] = '/content/old_data'"
      ],
      "metadata": {
        "id": "a7boNBB3Ab7_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_dataset_size():\n",
        "  os.makedirs(InputParams[\"NEW_DIR\"], exist_ok=True)\n",
        "  for dset in InputParams[\"DSET_NAMES\"]:\n",
        "\n",
        "    os.system(\"cp {}/*pkl {}\".format(InputParams[\"LONG_DIR\"], InputParams[\"NEW_DIR\"]))\n",
        "\n",
        "    print(\"Processing : old data\")\n",
        "    long_dset = nc.Dataset(os.path.join(InputParams[\"LONG_DIR\"], InputParams[\"Datafile\"]), \"r\")#add spectr file name\n",
        "    inds_dist = np.where(long_dset[\"distance\"][:]<InputParams[\"DISTANCE\"])[0]\n",
        "    new_len = len(inds_dist)\n",
        "\n",
        "    new_dset = nc.Dataset(os.path.join(InputParams[\"NEW_DIR\"], InputParams[\"Datafile\"]), \"w\")#add spectr name\n",
        "    new_dset.setncatts(long_dset.__dict__)\n",
        "    for group in long_dset.groups:\n",
        "            new_group = new_dset.createGroup(group)\n",
        "            for kattr in long_dset.groups[group].ncattrs():\n",
        "                vattr = long_dset.groups[group].getncattr(kattr)\n",
        "                setattr(new_group, kattr, vattr)\n",
        "    for name, dim in long_dset.dimensions.items():\n",
        "            new_dset.createDimension(name, len(dim) if not name==\"traces\" else new_len)\n",
        "    for name, var in long_dset.variables.items():\n",
        "            dims = tuple([new_dset.dimensions[dim].name for dim in var.dimensions])\n",
        "            x = new_dset.createVariable(name, var.datatype, dims)\n",
        "            new_dset[name].setncatts(long_dset[name].__dict__)\n",
        "    for i, ind in enumerate(inds_dist):\n",
        "            for name, var in new_dset.variables.items():\n",
        "                new_dset[name][i] = long_dset[name][ind]\n",
        "\n",
        "            if ((i+1)%(int(new_len/10)) == 0):\n",
        "                print(\"{} % done.\".format(round(100*(i+1)/new_len)))\n",
        "            if ((i+1)%(int(new_len/10)) == 0):\n",
        "                print(\"{} % done.\".format(round(100*(i+1)/new_len)))\n",
        "    long_dset.close()\n",
        "    new_dset.close()"
      ],
      "metadata": {
        "id": "-T4z7IfehXhu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import netCDF4 as nc\n",
        "reduce_dataset_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBUrnEuuAAr-",
        "outputId": "e542b408-f5c7-4925-bbad-10d83770be91"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing : old data\n",
            "10 % done.\n",
            "10 % done.\n",
            "20 % done.\n",
            "20 % done.\n",
            "30 % done.\n",
            "30 % done.\n",
            "40 % done.\n",
            "40 % done.\n",
            "50 % done.\n",
            "50 % done.\n",
            "60 % done.\n",
            "60 % done.\n",
            "70 % done.\n",
            "70 % done.\n",
            "80 % done.\n",
            "80 % done.\n",
            "90 % done.\n",
            "90 % done.\n",
            "100 % done.\n",
            "100 % done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/old_data\n"
      ],
      "metadata": {
        "id": "HS93lTTASwYH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if everything is correct\n",
        "datanew  = nc.Dataset('/content/red_data/dset_allspec_150_spectrograms.nc' , 'r')\n",
        "print(datanew)\n",
        "var1 = datanew.variables['mel_spectr']\n",
        "var2 = datanew.variables['class']\n",
        "var3 = datanew.variables['seis']\n",
        "var4 = datanew.variables['distance']\n",
        "print(var1.shape)\n",
        "datanew.close"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRVTXJv0GGUe",
        "outputId": "030c262d-8d1f-4a5a-e9c7-4fdc9a470dfd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'netCDF4._netCDF4.Dataset'>\n",
            "root group (NETCDF4 data model, file format HDF5):\n",
            "    dimensions(sizes): side(128), traces(16831), comp(3)\n",
            "    variables(dimensions): float32 mel_spectr(traces, side, side, comp), int32 class(traces), float32 distance(traces), int64 seis(traces)\n",
            "    groups: class_dict\n",
            "(16831, 128, 128, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Dataset.close of <class 'netCDF4._netCDF4.Dataset'>\n",
              "root group (NETCDF4 data model, file format HDF5):\n",
              "    dimensions(sizes): side(128), traces(16831), comp(3)\n",
              "    variables(dimensions): float32 mel_spectr(traces, side, side, comp), int32 class(traces), float32 distance(traces), int64 seis(traces)\n",
              "    groups: class_dict>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "InputParams = {}\n",
        "InputParams['DATA_PATH'] = '/content/red_data'\n",
        "### Dataset to use\n",
        "InputParams[\"DBASE_NAME\"] = \"dset_allspec_150_spectrograms.nc\"\n",
        "\n",
        "### Size of test and validation set.\n",
        "InputParams[\"VAL_SIZE\"] = 0.1\n",
        "InputParams[\"TEST_SIZE\"] = 0.1\n",
        "InputParams[\"stations_subset\"] = [\"ETA00\", \"NNL62\", \"NTA02\", \"STA02\", \"NWP05\", \"WTA00\"]"
      ],
      "metadata": {
        "id": "gyes04HTERbM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_station_time():\n",
        "  dbase = Dataset(os.path.join(InputParams[\"DATA_PATH\"], InputParams[\"DBASE_NAME\"]) , 'r')\n",
        "  dbase_size = dbase.dimensions[\"traces\"].size\n",
        "  indices = np.asarray(range(dbase_size))\n",
        "  path = os.path.join(InputParams[\"DATA_PATH\"], \"seis_dict.pkl\")\n",
        "  seis_dict = load_obj(path)\n",
        "  seis_subset = [seis_dict[seis] for seis in InputParams[\"stations_subset\"]]\n",
        "  inds_seis = np.array([ind for ind in indices if dbase[\"seis\"][ind] in seis_subset])\n",
        "  sub_dbase = dbase[\"class\"][inds_seis]\n",
        "  classes = np.unique(sub_dbase)\n",
        "  inds_test = []\n",
        "  inds_val = []\n",
        "  inds_train = []\n",
        "  end_test = int(InputParams[\"TEST_SIZE\"]*len(inds_seis))\n",
        "  end_val = end_test + int(InputParams[\"VAL_SIZE\"]*len(inds_seis))\n",
        "  test = inds_seis[:end_test]\n",
        "  val = inds_seis[end_test:end_val]\n",
        "  train = inds_seis[end_val:]\n",
        "  means = np.mean(dbase[\"mel_spectr\"][train], axis=(0, 1, 2))\n",
        "  stds = np.std(dbase[\"mel_spectr\"][train], axis=(0, 1, 2))\n",
        "  np.save(os.path.join(InputParams[\"DATA_PATH\"], \"mean_spectr_station_time.npy\"), means.data)\n",
        "  np.save(os.path.join(InputParams[\"DATA_PATH\"], \"std_spectr_station_time.npy\"), stds.data)\n",
        "  np.save(os.path.join(InputParams[\"DATA_PATH\"], \"test_station_indices.npy\"), test.data)\n",
        "  np.save(os.path.join(InputParams[\"DATA_PATH\"], \"val_station_indices.npy\"), val.data)\n",
        "  np.save(os.path.join(InputParams[\"DATA_PATH\"], \"train_station_indices.npy\"), train.data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CP9qvx8ihIHx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "jSnd5UyHJE8L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_by_station_time()"
      ],
      "metadata": {
        "id": "d12MVieNIOrL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = np.load('/content/red_data/train_station_indices.npy')\n",
        "val = np.load('/content/red_data/val_station_indices.npy')\n",
        "test = np.load('/content/red_data/test_station_indices.npy')"
      ],
      "metadata": {
        "id": "95ZOzZwDJI5x"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making DataLoader Classes"
      ],
      "metadata": {
        "id": "BT4hnOoJJ-uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "etQR-BoKK8Ww"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "marXs86bOzxc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {}\n",
        "cfg[\"DataDir\"] =  '/content/red_data'\n",
        "cfg[\"Datafile\"] = 'dset_allspec_150_spectrograms.nc'"
      ],
      "metadata": {
        "id": "7YLJzp7rLPD3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "JhaitgOZ1W0C"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class spect_dataset(Dataset):\n",
        "  def __init__(self , phase ,  InputParams = cfg ):\n",
        "    \"\"\"Loads dataset, indices, and normalisation.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "\n",
        "        phase: str\n",
        "            \"train\" or \"val\" phase\n",
        "        \"\"\"\n",
        "    self.dset = nc.Dataset(os.path.join(InputParams[\"DataDir\"] , InputParams[\"Datafile\"]), 'r')\n",
        "    self.inds = np.load(os.path.join( cfg[\"DataDir\"]  ,\"%s_station_indices.npy\"%(phase)))\n",
        "\n",
        "    self.means = np.load(os.path.join( cfg[\"DataDir\"]  ,(\"mean_spectr_station_time.npy\")))\n",
        "    self.stds = np.load(os.path.join( cfg[\"DataDir\"]  ,(\"std_spectr_station_time.npy\")))\n",
        "    self.phase = phase\n",
        "    _, self.class_counts_all = np.unique(self.dset[\"class\"][self.inds], return_counts=True)\n",
        "    self.binary = True\n",
        "    self.class_dict_all = {}\n",
        "    for species in self.dset.groups[\"class_dict\"].ncattrs():\n",
        "      self.class_dict_all[self.dset.groups[\"class_dict\"].getncattr(species)] = species\n",
        "    self.class_counts = []\n",
        "    self.class_dict = {}\n",
        "\n",
        "    cls_el = self.dset.groups[\"class_dict\"].getncattr(\"ELEPHANTIDAE\")\n",
        "    self.class_counts.append(self.class_counts_all[cls_el])\n",
        "    non_el_class_counts = [count for i,count in enumerate(self.class_counts_all) if i!=cls_el]\n",
        "    self.class_counts.append(np.sum(non_el_class_counts))\n",
        "    self.class_dict[0] = \"ELEPHANTIDAE\"\n",
        "    self.class_dict[1] = \"OTHERS\"\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.inds)\n",
        "  def returncount(self):\n",
        "    return self.class_counts\n",
        "  def returndict(self):\n",
        "    return self.class_dict\n",
        "  def __getitem__(self , index):\n",
        "\n",
        "    ind = self.inds[index]\n",
        "    cls = self.dset[\"class\"][ind]\n",
        "    if cls == 1 :\n",
        "      cls = 1\n",
        "    else :\n",
        "      cls = 0\n",
        "    spectr = (self.dset[\"mel_spectr\"][ind] - self.means)/self.stds\n",
        "    spectr = np.transpose(spectr, (2, 0, 1))\n",
        "    scale_factor = 224 / 128\n",
        "    x = torch.from_numpy(np.reshape(spectr[0] , (1 , 1,128,128)))\n",
        "    upsampled_x = F.interpolate(x, scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
        "\n",
        "    return torch.reshape((torch.squeeze(upsampled_x)) , (1,224,224)), torch.from_numpy(np.array([cls]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KlmMcnd_KLM2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spect_dataset( \"train\" , cfg)"
      ],
      "metadata": {
        "id": "mEsBZKzvPeYf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.returncount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syVOFhG5QIkW",
        "outputId": "507c4316-19a1-44cb-855e-b254f9dcff31"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5579, 2018]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFTCeu50jjnc",
        "outputId": "be459302-2599-4c6d-fdc0-7d87d839065f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.class_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OcDlfWrt1EL",
        "outputId": "1e7d7cd1-3476-4cae-ce17-3ff20b5dad73"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5579, 2018]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "\n",
        "class PretrainedCNN(nn.Module):\n",
        "    \"\"\"CNN pretrained on Imagenet.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, class_counts, init_bias=False, arch=\"resnet\", pretrained=True):\n",
        "        \"\"\" Add classification head to pretrained CNN.\"\"\"\n",
        "\n",
        "        super(PretrainedCNN, self).__init__()\n",
        "\n",
        "        self.cnnlayer = nn.Conv2d(1 , 3 , kernel_size = (1,1) , stride = (1,1))\n",
        "\n",
        "        self.model = models.squeezenet1_0(pretrained=True)\n",
        "        self.model.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1,1), stride=(1,1))\n",
        "        self.model.num_classes = 2\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "\n",
        "        x = self.cnnlayer(x)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "1OxL2w4RVQ2j"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, optimizer, criterion, inputs, labels, device, train_conf_matrix):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.long().squeeze().to(device)\n",
        "    preds = model(inputs)\n",
        "    loss = criterion(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    class_preds = torch.argmax(preds, dim=1)\n",
        "\n",
        "    conf_batch = confusion_matrix(labels.detach().cpu().numpy(), class_preds.detach().cpu().numpy(), labels=[0,1])\n",
        "    train_conf_matrix[:][:] += conf_batch\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "pwa7UjmTQvJf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_step(model, criterion, inputs, labels, device, val_conf_matrix):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.long().squeeze().to(device)\n",
        "        preds = model(inputs)\n",
        "        loss = criterion(preds, labels)\n",
        "\n",
        "    class_preds = torch.argmax(preds, dim=1)\n",
        "\n",
        "    conf_batch = confusion_matrix(labels.detach().cpu().numpy(), class_preds.detach().cpu().numpy(), labels=[0,1])\n",
        "    val_conf_matrix[:][:] += conf_batch\n",
        "\n",
        "    return loss.item()\n",
        ""
      ],
      "metadata": {
        "id": "VSYIZipDVED2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import copy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.optim import lr_scheduler"
      ],
      "metadata": {
        "id": "cM6_F3--YVtH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg['batch_size'] = 32\n",
        "cfg['lr'] = 0.00001\n",
        "cfg['l2reg'] = 0.1\n",
        "cfg['scheduler'] = {}\n",
        "cfg['scheduler']['step_size'] = 10\n",
        "cfg['scheduler']['gamma'] = 1\n",
        "cfg['epochs'] = 20\n",
        "cfg['patience'] = 20\n"
      ],
      "metadata": {
        "id": "TDQbFrkLbxF-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setupandtrain(cfg):\n",
        "  np.random.seed(42)\n",
        "  torch.manual_seed(42)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "  k_datasets = {x: spect_dataset(x , cfg) for x in ['train', 'val', 'test']}\n",
        "  k_dataloaders = {x: torch.utils.data.DataLoader(k_datasets[x], batch_size=cfg[\"batch_size\"], shuffle=True) for x in ['train', 'val', 'test']}\n",
        "  dataset_sizes = {x: len(k_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "  class_counts = k_datasets[\"train\"].class_counts\n",
        "  tot_cls = np.sum(class_counts)\n",
        "  class_dict = k_datasets[\"train\"].class_dict\n",
        "  num_classes = len(class_counts)\n",
        "  class_counts_val = k_datasets[\"val\"].class_counts\n",
        "  tot_cls_val = np.sum(class_counts_val)\n",
        "\n",
        "  model = PretrainedCNN(num_classes , class_counts, init_bias=False, arch='resnet', pretrained=True)\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr = cfg['lr'], weight_decay=cfg['l2reg'])\n",
        "  weights = torch.Tensor([tot_cls/class_count for class_count in class_counts]).to(device)\n",
        "  total_weights = torch.sum(weights)\n",
        "  weights = weights/total_weights\n",
        "\n",
        "\n",
        "\n",
        "  criterion_train = nn.CrossEntropyLoss(weight=weights)\n",
        "  criterion_val = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  scheduler = lr_scheduler.StepLR(optimizer, step_size=cfg[\"scheduler\"][\"step_size\"], gamma=cfg[\"scheduler\"][\"gamma\"])\n",
        "  print(cfg, flush=True)\n",
        "\n",
        "  logger = Logger(cfg)\n",
        "\n",
        "  train_loss = 0\n",
        "  val_loss = 0\n",
        "  species = [class_dict[i] for i in range(num_classes)]\n",
        "\n",
        "\n",
        "  val_conf_matrix = pd.DataFrame(data = np.zeros((num_classes, num_classes)), columns = species)\n",
        "  train_conf_matrix = pd.DataFrame(data = np.zeros((num_classes, num_classes)), columns = species)\n",
        "  test_conf_matrix = pd.DataFrame(data = np.zeros((num_classes, num_classes)), columns = species)\n",
        "\n",
        "\n",
        "  val_conf_matrix.index = val_conf_matrix.columns\n",
        "  train_conf_matrix.index = train_conf_matrix.columns\n",
        "  test_conf_matrix.index = test_conf_matrix.columns\n",
        "\n",
        "\n",
        "\n",
        "  best_val_conf_matrix = copy.deepcopy(val_conf_matrix)\n",
        "  best_model_weights = copy.deepcopy(model.state_dict())\n",
        "  early_stopping = EarlyStopping(patience=cfg['patience'])\n",
        "\n",
        "\n",
        "  #output untrained model metrics\n",
        "  model.eval()\n",
        "  for inputs, labels in k_dataloaders[\"train\"]:\n",
        "    batch_loss = val_step(model, criterion_val, inputs, labels, device, train_conf_matrix)\n",
        "    train_loss += batch_loss\n",
        "  for inputs, labels in k_dataloaders[\"val\"]:\n",
        "    batch_loss = val_step(model, criterion_val, inputs, labels, device, val_conf_matrix)\n",
        "    val_loss += batch_loss\n",
        "  train_loss = train_loss/dataset_sizes[\"train\"]\n",
        "  val_loss = val_loss/dataset_sizes[\"val\"]\n",
        "  logger.log(train_loss, val_loss, train_conf_matrix, val_conf_matrix, 0)\n",
        "  print(f\" Epochs 0 / {cfg['epochs']} , train_loss : {train_loss} , val_loss : {val_loss} \")\n",
        "  print(val_conf_matrix, flush=True)\n",
        "  print(\"\\n\", flush=True)\n",
        "\n",
        "\n",
        "  #reset metrics\n",
        "\n",
        "  val_loss = 0\n",
        "  train_loss = 0\n",
        "  val_conf_matrix[:][:] = np.zeros((num_classes, num_classes))\n",
        "  train_conf_matrix[:][:] = np.zeros((num_classes, num_classes))\n",
        "  for epoch in range(1,cfg['epochs']+1):\n",
        "    model.train()\n",
        "    for inputs, labels in k_dataloaders[\"train\"]:\n",
        "      batch_loss = train_step(model, optimizer, criterion_val , inputs, labels, device, train_conf_matrix)\n",
        "      train_loss += batch_loss\n",
        "    model.eval()\n",
        "    for inputs, labels in k_dataloaders[\"val\"]:\n",
        "        batch_loss = val_step(model, criterion_val, inputs, labels, device, val_conf_matrix)\n",
        "        val_loss += batch_loss\n",
        "    # End of epoch callbacks\n",
        "    scheduler.step()\n",
        "    best_val_conf_matrix, best_model_weights = update_best_model(val_conf_matrix, best_val_conf_matrix, model, best_model_weights)\n",
        "    early_stopping.early_stop(val_conf_matrix)\n",
        "    train_loss = train_loss/dataset_sizes[\"train\"]\n",
        "    val_loss = val_loss/dataset_sizes[\"val\"]\n",
        "    logger.log(train_loss, val_loss, train_conf_matrix, val_conf_matrix, epoch)\n",
        "    print(\"Epoch {}/{}, train loss = {}, valloss = {}\".format(epoch, cfg['epochs'], train_loss, val_loss), flush=True)\n",
        "    print(val_conf_matrix, flush=True)\n",
        "    print(\"\\n\", flush=True)\n",
        "\n",
        "    if early_stopping.stop:\n",
        "      break\n",
        "\n",
        "    val_loss = 0\n",
        "    train_loss = 0\n",
        "    val_conf_matrix[:][:] = np.zeros((num_classes, num_classes))\n",
        "    train_conf_matrix[:][:] = np.zeros((num_classes, num_classes))\n",
        "  print(\"Best validation confusion matrix: \\n\", flush=True)\n",
        "  print(best_val_conf_matrix, flush=True)\n",
        "\n",
        "  model.load_state_dict(best_model_weights)\n",
        "  for inputs, labels in k_dataloaders[\"test\"]:\n",
        "    _ = val_step(model, criterion, inputs, labels, device, test_conf_matrix)\n",
        "  print(\"Test confusion matrix from best val model: \\n\", flush=True)\n",
        "  print(test_conf_matrix, flush=True)\n",
        "  #_, _ = logger.get_all_labels_probas(model, loader_val_noshuffle, device, phase=\"val\", save=True)\n",
        "  #_, _ = logger.get_all_labels_probas(model, loader_train_noshuffle, device, phase=\"train\", save=True)\n",
        "  #_, _ = logger.get_all_labels_probas(model, loader_test_noshuffle, device, phase=\"test\", save=True)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "ZLqACuFrYhLy"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setupandtrain(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i3Fvxw9Nk-G",
        "outputId": "d702d19e-60d3-4afd-ba75-788c80e5f2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "{'DataDir': '/content/red_data', 'Datafile': 'dset_allspec_150_spectrograms.nc', 'batch_size': 32, 'lr': 1e-05, 'l2reg': 0.1, 'scheduler': {'step_size': 10, 'gamma': 1}, 'epochs': 20, 'patience': 20}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epochs 0 / 20 , train_loss : 0.5699021503990287 , val_loss : 0.51136889924993 \n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE           5.0   156.0\n",
            "OTHERS                 8.0   780.0\n",
            "\n",
            "\n",
            "Epoch 1/20, train loss = 0.5122108514079518, valloss = 0.38018229236341505\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE          23.0   138.0\n",
            "OTHERS                 8.0   780.0\n",
            "\n",
            "\n",
            "Epoch 2/20, train loss = 0.41087454156497755, valloss = 0.31149578446206355\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE          43.0   118.0\n",
            "OTHERS                 4.0   784.0\n",
            "\n",
            "\n",
            "Epoch 3/20, train loss = 0.3552702177790132, valloss = 0.26868147242056684\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE          59.0   102.0\n",
            "OTHERS                10.0   778.0\n",
            "\n",
            "\n",
            "Epoch 4/20, train loss = 0.3138694104762929, valloss = 0.23148126295670568\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE          73.0    88.0\n",
            "OTHERS                 5.0   783.0\n",
            "\n",
            "\n",
            "Epoch 5/20, train loss = 0.2991691373972198, valloss = 0.22459234000508477\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         122.0    39.0\n",
            "OTHERS                25.0   763.0\n",
            "\n",
            "\n",
            "Epoch 6/20, train loss = 0.2811380338085095, valloss = 0.20595356033775403\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         102.0    59.0\n",
            "OTHERS                19.0   769.0\n",
            "\n",
            "\n",
            "Epoch 7/20, train loss = 0.26167514130553554, valloss = 0.19085768766473543\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         121.0    40.0\n",
            "OTHERS                17.0   771.0\n",
            "\n",
            "\n",
            "Epoch 8/20, train loss = 0.24919739977158856, valloss = 0.19185898628074075\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         132.0    29.0\n",
            "OTHERS                21.0   767.0\n",
            "\n",
            "\n",
            "Epoch 9/20, train loss = 0.23990516294158257, valloss = 0.18680861727329653\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         122.0    39.0\n",
            "OTHERS                16.0   772.0\n",
            "\n",
            "\n",
            "Epoch 10/20, train loss = 0.23798401700645366, valloss = 0.1937840031371604\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE          95.0    66.0\n",
            "OTHERS                11.0   777.0\n",
            "\n",
            "\n",
            "Epoch 11/20, train loss = 0.22352647230595965, valloss = 0.18790351276779577\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         136.0    25.0\n",
            "OTHERS                22.0   766.0\n",
            "\n",
            "\n",
            "Epoch 12/20, train loss = 0.21260231898868806, valloss = 0.17126342016728585\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         131.0    30.0\n",
            "OTHERS                16.0   772.0\n",
            "\n",
            "\n",
            "Epoch 13/20, train loss = 0.2044540713833589, valloss = 0.21247479385520684\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         145.0    16.0\n",
            "OTHERS                33.0   755.0\n",
            "\n",
            "\n",
            "Epoch 14/20, train loss = 0.19827871550474258, valloss = 0.16409881260673917\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         140.0    21.0\n",
            "OTHERS                20.0   768.0\n",
            "\n",
            "\n",
            "Epoch 15/20, train loss = 0.191537758433789, valloss = 0.17130741656014992\n",
            "              ELEPHANTIDAE  OTHERS\n",
            "ELEPHANTIDAE         132.0    29.0\n",
            "OTHERS                18.0   770.0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Example path to your .nc file\n",
        "nc_file_path = '/content/red_data/dset_allspec_150_spectrograms.nc'\n",
        "os.makedirs('/content/drive/My Drive/8julyseis' , exist_ok= True)\n",
        "# Destination folder in your Google Drive\n",
        "drive_folder = '/content/drive/My Drive/8julyseis/spect.nc'\n",
        "\n",
        "# Extracting file name\n",
        "file_name = os.path.basename(nc_file_path)\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "shutil.copy(nc_file_path, drive_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8sZMs1ff3_HX",
        "outputId": "eb0b214a-af61-4dea-f821-7bc0263ebef9"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/8julyseis/spect.nc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping :\n",
        "  #early stopping if accuracy does not improve\n",
        "  def __init__(self , patience = 5):\n",
        "    self.stop = False\n",
        "    self.patience = max(patience , 1)\n",
        "    self.counter = 0\n",
        "    self.best_acc = 0\n",
        "  def early_stop(self , conf_mat ):\n",
        "    avg_accuracy = np.mean(np.diag(conf_mat) / np.sum(conf_mat, axis=1))\n",
        "    if self.best_acc < avg_accuracy :\n",
        "      self.best_acc = avg_accuracy\n",
        "      self.counter = 0\n",
        "    else :\n",
        "      self.counter += 1\n",
        "    if self.counter >= self.patience :\n",
        "      self.stop = True\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "bFMJIVaIceOf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_best_model(conf_matrix, best_conf_matrix, model, best_model_weights):\n",
        "  avg_accuracy = np.mean(np.diag(conf_matrix)/np.sum(conf_matrix, axis=1))\n",
        "  if np.sum(np.sum(best_conf_matrix)) == 0:\n",
        "    best_avg_accuracy = 0\n",
        "  else :\n",
        "    best_avg_accuracy = np.mean(np.diag(best_conf_matrix)/np.sum(best_conf_matrix, axis=1))\n",
        "  if avg_accuracy>best_avg_accuracy:\n",
        "        best_model_weights = copy.deepcopy(model.state_dict())\n",
        "        best_conf_matrix = copy.deepcopy(conf_matrix)\n",
        "  return best_conf_matrix, best_model_weights"
      ],
      "metadata": {
        "id": "P72AZjG-ceW0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_best_model(model, best_model_weights, cfg):\n",
        "  model.load_state_dict(best_model_weights)\n",
        "  torch.save(model, \"best_model.pt\")"
      ],
      "metadata": {
        "id": "iVfinMhm5DIC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, cfg, log_csv=True, log_tboard=True, metrics=[\"tp\", \"fp\", \"tn\", \"fn\", \"avg_acc\", \"acc\", \"mcc\", \"loss\", \"f1\", \"tpr\", \"fpr\", \"tnr\", \"precision\"]):\n",
        "        self.metrics = metrics\n",
        "        self.log_csv = log_csv\n",
        "        self.log_tboard = log_tboard\n",
        "        if log_csv:\n",
        "            self.df_log_train = pd.DataFrame(data=np.zeros((cfg['epochs']+1, len(metrics))), columns=metrics)\n",
        "            self.df_log_val = pd.DataFrame(data=np.zeros((cfg['epochs']+1 , len(metrics))), columns=metrics)\n",
        "\n",
        "    def log(self, train_loss, val_loss, train_conf_matrix, val_conf_matrix, epoch):\n",
        "        for metric in self.metrics:\n",
        "            if self.log_csv:\n",
        "                self.df_log_train[metric][epoch] = self.compute_metric(metric, train_loss, train_conf_matrix)\n",
        "                self.df_log_val[metric][epoch] = self.compute_metric(metric, val_loss, val_conf_matrix)\n",
        "\n",
        "    def compute_metric(self, metric, loss, conf_matrix):\n",
        "        \"\"\"Return metric of interest.\"\"\"\n",
        "        tp = conf_matrix.iloc[0,0]\n",
        "        fp = conf_matrix.iloc[1,0]\n",
        "        tn = conf_matrix.iloc[1,1]\n",
        "        fn = conf_matrix.iloc[0,1]\n",
        "        tpr = tp / (tp + fn)\n",
        "        tnr = tn / (tn + fp)\n",
        "        fpr = fp / (fp + tn)\n",
        "        precision = tp / (tp + fp)\n",
        "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "        bal_acc = (tpr + tnr) / 2\n",
        "        f1 = 2 * tp / (2 * tp + fn + fp)\n",
        "\n",
        "        try:\n",
        "            mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "        except RuntimeWarning:\n",
        "            mcc = 0\n",
        "\n",
        "        if metric == \"tp\":\n",
        "            return tp\n",
        "        elif metric == \"fp\":\n",
        "            return fp\n",
        "        elif metric == \"tn\":\n",
        "            return tn\n",
        "        elif metric == \"fn\":\n",
        "            return fn\n",
        "        elif metric == \"tpr\":\n",
        "            return tpr\n",
        "        elif metric == \"tnr\":\n",
        "            return tnr\n",
        "        elif metric == \"fpr\":\n",
        "            return fpr\n",
        "        elif metric == \"precision\":\n",
        "            return precision\n",
        "        elif metric == \"acc\":\n",
        "            return acc\n",
        "        elif metric == \"avg_acc\":\n",
        "            return bal_acc\n",
        "        elif metric == \"f1\":\n",
        "            return f1\n",
        "        elif metric == \"mcc\":\n",
        "            return mcc\n",
        "        elif metric == \"loss\":\n",
        "            return loss\n",
        "        else:\n",
        "            raise ValueError(\"Metric not implemented.\")\n",
        "\n",
        "    def log_weights(self, weights):\n",
        "        \"\"\"write class weights to info file\"\"\"\n",
        "        with open(\"class_weights.txt\", \"w\") as f:\n",
        "            f.write(\"Class weights in training set:\\n\")\n",
        "            f.write(str(weights.detach().cpu().numpy()))\n",
        "\n",
        "    def log_model(self, model, dataset):\n",
        "        \"\"\"write model summary to file.\"\"\"\n",
        "        torch.save(model, '/content/drive/MyDrive/SecondFinal/model.pth')\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Save csv file and close tensorboard summaries in destructor\"\"\"\n",
        "        if self.log_csv:\n",
        "            self.df_log_train.to_csv(\"train_metrics_log.csv\")\n",
        "            self.df_log_val.to_csv(\"val_metrics_log.csv\")\n",
        "\n",
        "    def get_all_labels_probas(self, model, dataloader, device, phase=\"val\", save=True):\n",
        "        \"\"\"loop through val set and return all labels and predictions probas.\"\"\"\n",
        "        model.eval()\n",
        "        tot_labels = []\n",
        "        tot_preds = []\n",
        "        for inputs, labels, _ in dataloader:\n",
        "            with torch.no_grad():\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                preds = torch.nn.functional.softmax(model(inputs), dim=-1)\n",
        "                tot_labels.append(labels.detach().cpu().numpy())\n",
        "                tot_preds.append(preds.detach().cpu().numpy())\n",
        "\n",
        "        tot_labels = np.concatenate(tot_labels, axis=0)\n",
        "        tot_preds = np.concatenate(tot_preds, axis=0)\n",
        "\n",
        "        if save:\n",
        "            np.save(\"{}_all_labels.npy\".format(phase), tot_labels)\n",
        "            np.save(\"{}_all_probas.npy\".format(phase), tot_preds)\n",
        "\n",
        "        return tot_labels, tot_preds\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "XtCczk4x55v1"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}